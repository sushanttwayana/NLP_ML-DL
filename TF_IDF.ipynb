{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanttwayana/NLP_ML-DL/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-21T12:15:41.012531Z",
          "iopub.execute_input": "2024-02-21T12:15:41.012942Z",
          "iopub.status.idle": "2024-02-21T12:15:55.901181Z",
          "shell.execute_reply.started": "2024-02-21T12:15:41.012911Z",
          "shell.execute_reply": "2024-02-21T12:15:55.899596Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWaAFoLWoGAJ",
        "outputId": "61bccffe-dffa-433c-845c-7c9c71988680"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade nltk\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:15:55.903949Z",
          "iopub.execute_input": "2024-02-21T12:15:55.904461Z",
          "iopub.status.idle": "2024-02-21T12:16:10.964140Z",
          "shell.execute_reply.started": "2024-02-21T12:15:55.904393Z",
          "shell.execute_reply": "2024-02-21T12:16:10.962603Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RIR3MFZoGAL",
        "outputId": "5adead8a-dfa1-4721-ea39-661cff2e5202"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "One Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda. It\n",
        "has been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since July 1997, with its\n",
        "individual chapters compiled in 107 tankōbon volumes as of November 2023. The story follows the\n",
        "adventures of Monkey D. Luffy and his crew, the Straw Hat Pirates, where he explores the Grand Line\n",
        "in search of the mythical treasure known as the \"One Piece\" in order to become the next King of the\n",
        "Pirates. The manga spawned a media franchise, having been adapted into a festival film by Production\n",
        "I.G, and an anime series by Toei Animation, which began broadcasting in 1999. Additionally, Toei has\n",
        "developed fourteen animated feature films, one original video animation, and thirteen television\n",
        "specials. Several companies have developed various types of merchandising and media, such as a trading\n",
        "card game and numerous video games. The manga series was licensed for an English language release in\n",
        "North America and the United Kingdom by Viz Media and in Australia by Madman Entertainment.\n",
        "The anime series was licensed by 4Kids Entertainment for an English-language release in North America\n",
        "in 2004 before the license was dropped and subsequently acquired by Funimation in 2007.\"\"\"\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:10.966134Z",
          "iopub.execute_input": "2024-02-21T12:16:10.966534Z",
          "iopub.status.idle": "2024-02-21T12:16:10.974293Z",
          "shell.execute_reply.started": "2024-02-21T12:16:10.966498Z",
          "shell.execute_reply": "2024-02-21T12:16:10.972785Z"
        },
        "trusted": true,
        "id": "wAOQ20ySoGAM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:10.978147Z",
          "iopub.execute_input": "2024-02-21T12:16:10.978941Z",
          "iopub.status.idle": "2024-02-21T12:16:10.989316Z",
          "shell.execute_reply.started": "2024-02-21T12:16:10.978896Z",
          "shell.execute_reply": "2024-02-21T12:16:10.988049Z"
        },
        "trusted": true,
        "id": "OrzDFQaRoGAM",
        "outputId": "9d1e440a-6f42-4cb9-b1fa-8cc091996846"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\nOne Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda. It\\nhas been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since July 1997, with its \\nindividual chapters compiled in 107 tankōbon volumes as of November 2023. The story follows the \\nadventures of Monkey D. Luffy and his crew, the Straw Hat Pirates, where he explores the Grand Line \\nin search of the mythical treasure known as the \"One Piece\" in order to become the next King of the \\nPirates. The manga spawned a media franchise, having been adapted into a festival film by Production \\nI.G, and an anime series by Toei Animation, which began broadcasting in 1999. Additionally, Toei has\\ndeveloped fourteen animated feature films, one original video animation, and thirteen television \\nspecials. Several companies have developed various types of merchandising and media, such as a trading\\ncard game and numerous video games. The manga series was licensed for an English language release in \\nNorth America and the United Kingdom by Viz Media and in Australia by Madman Entertainment. \\nThe anime series was licensed by 4Kids Entertainment for an English-language release in North America \\nin 2004 before the license was dropped and subsequently acquired by Funimation in 2007.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# PorterStemmer -> for stemmering purpose\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:10.990831Z",
          "iopub.execute_input": "2024-02-21T12:16:10.991918Z",
          "iopub.status.idle": "2024-02-21T12:16:10.998806Z",
          "shell.execute_reply.started": "2024-02-21T12:16:10.991880Z",
          "shell.execute_reply": "2024-02-21T12:16:10.997733Z"
        },
        "trusted": true,
        "id": "dvR-F201oGAN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokernization -> converts paragraphs into sentences-words\n",
        "\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.000146Z",
          "iopub.execute_input": "2024-02-21T12:16:11.000916Z",
          "iopub.status.idle": "2024-02-21T12:16:11.012559Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.000880Z",
          "shell.execute_reply": "2024-02-21T12:16:11.010423Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpOoaE-KoGAN",
        "outputId": "c4a64e0a-76b6-4f06-c172-b5f70120b3c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.014403Z",
          "iopub.execute_input": "2024-02-21T12:16:11.015298Z",
          "iopub.status.idle": "2024-02-21T12:16:11.025995Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.015263Z",
          "shell.execute_reply": "2024-02-21T12:16:11.024957Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Kx_5vZoGAN",
        "outputId": "4f36691a-b3dd-457b-846e-997550a05a1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nOne Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda.', \"It\\nhas been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since July 1997, with its \\nindividual chapters compiled in 107 tankōbon volumes as of November 2023.\", 'The story follows the \\nadventures of Monkey D. Luffy and his crew, the Straw Hat Pirates, where he explores the Grand Line \\nin search of the mythical treasure known as the \"One Piece\" in order to become the next King of the \\nPirates.', 'The manga spawned a media franchise, having been adapted into a festival film by Production \\nI.G, and an anime series by Toei Animation, which began broadcasting in 1999.', 'Additionally, Toei has\\ndeveloped fourteen animated feature films, one original video animation, and thirteen television \\nspecials.', 'Several companies have developed various types of merchandising and media, such as a trading\\ncard game and numerous video games.', 'The manga series was licensed for an English language release in \\nNorth America and the United Kingdom by Viz Media and in Australia by Madman Entertainment.', 'The anime series was licensed by 4Kids Entertainment for an English-language release in North America \\nin 2004 before the license was dropped and subsequently acquired by Funimation in 2007.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.027527Z",
          "iopub.execute_input": "2024-02-21T12:16:11.028152Z",
          "iopub.status.idle": "2024-02-21T12:16:11.037489Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.028119Z",
          "shell.execute_reply": "2024-02-21T12:16:11.036675Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jyQaKZFoGAO",
        "outputId": "b938a864-7a0f-4bdf-eaa5-307b518e6f2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply stemming\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.038617Z",
          "iopub.execute_input": "2024-02-21T12:16:11.039232Z",
          "iopub.status.idle": "2024-02-21T12:16:11.056342Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.039201Z",
          "shell.execute_reply": "2024-02-21T12:16:11.055211Z"
        },
        "trusted": true,
        "id": "FafKHGSQoGAO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('history')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.060519Z",
          "iopub.execute_input": "2024-02-21T12:16:11.061906Z",
          "iopub.status.idle": "2024-02-21T12:16:11.069678Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.061861Z",
          "shell.execute_reply": "2024-02-21T12:16:11.068519Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6cXagr-XoGAO",
        "outputId": "fe8e3ff8-6f02-4ba8-b443-143a1510b702"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'histori'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:16:11.071724Z",
          "iopub.execute_input": "2024-02-21T12:16:11.072216Z",
          "iopub.status.idle": "2024-02-21T12:16:11.082149Z",
          "shell.execute_reply.started": "2024-02-21T12:16:11.072171Z",
          "shell.execute_reply": "2024-02-21T12:16:11.080771Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTzooDeDoGAO",
        "outputId": "ae07e911-ef3f-411f-f3b1-b60ca83f6e0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('/usr/share/nltk_data')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:21:53.435528Z",
          "iopub.execute_input": "2024-02-21T12:21:53.435925Z",
          "iopub.status.idle": "2024-02-21T12:21:53.444243Z",
          "shell.execute_reply.started": "2024-02-21T12:21:53.435897Z",
          "shell.execute_reply": "2024-02-21T12:21:53.443048Z"
        },
        "trusted": true,
        "id": "JkOuncpLoGAO",
        "outputId": "18743974-beb7-4f9f-e521-451002fec315"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Error loading /usr/share/nltk_data: Package\n[nltk_data]     '/usr/share/nltk_data' not found in index\n",
          "output_type": "stream"
        },
        {
          "execution_count": 67,
          "output_type": "execute_result",
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lematization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('goes')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:23:44.385389Z",
          "iopub.execute_input": "2024-02-21T12:23:44.385791Z",
          "iopub.status.idle": "2024-02-21T12:23:44.544641Z",
          "shell.execute_reply.started": "2024-02-21T12:23:44.385762Z",
          "shell.execute_reply": "2024-02-21T12:23:44.542681Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6ZWToeK5oGAP",
        "outputId": "9f406e80-73dd-46d7-9a3e-f1336732adb6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-21T12:18:15.612462Z",
          "iopub.execute_input": "2024-02-21T12:18:15.613028Z",
          "iopub.status.idle": "2024-02-21T12:18:15.619900Z",
          "shell.execute_reply.started": "2024-02-21T12:18:15.612972Z",
          "shell.execute_reply": "2024-02-21T12:18:15.618687Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVrtvUnIoGAP",
        "outputId": "c4a5e57a-fac3-4af8-d755-8efd0acbfea6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TsLTx7PpSZz",
        "outputId": "d2a68b0d-f0d0-4027-a4de-e06a6c0f3df6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  review = review.lower()\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "1wShb7M1oGAP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIu806EFpxxB",
        "outputId": "89da0db1-9b5b-45b1-879d-e6ec3f8f9f35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' one piece  stylized in all caps  is a japanese manga series written and illustrated by eiichiro oda ',\n",
              " 'it has been serialized in shueisha s sh nen manga magazine weekly sh nen jump since july       with its  individual chapters compiled in     tank bon volumes as of november      ',\n",
              " 'the story follows the  adventures of monkey d  luffy and his crew  the straw hat pirates  where he explores the grand line  in search of the mythical treasure known as the  one piece  in order to become the next king of the  pirates ',\n",
              " 'the manga spawned a media franchise  having been adapted into a festival film by production  i g  and an anime series by toei animation  which began broadcasting in      ',\n",
              " 'additionally  toei has developed fourteen animated feature films  one original video animation  and thirteen television  specials ',\n",
              " 'several companies have developed various types of merchandising and media  such as a trading card game and numerous video games ',\n",
              " 'the manga series was licensed for an english language release in  north america and the united kingdom by viz media and in australia by madman entertainment ',\n",
              " 'the anime series was licensed by  kids entertainment for an english language release in north america  in      before the license was dropped and subsequently acquired by funimation in      ']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stemming\n",
        "nltk.download('stopwords')\n",
        "\n",
        "for i in corpus :\n",
        "\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfG5_nqOp2JL",
        "outputId": "4f0c923c-1176-4cd5-8746-fa1731bb60e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one\n",
            "piec\n",
            "styliz\n",
            "cap\n",
            "japanes\n",
            "manga\n",
            "seri\n",
            "written\n",
            "illustr\n",
            "eiichiro\n",
            "oda\n",
            "serial\n",
            "shueisha\n",
            "sh\n",
            "nen\n",
            "manga\n",
            "magazin\n",
            "weekli\n",
            "sh\n",
            "nen\n",
            "jump\n",
            "sinc\n",
            "juli\n",
            "individu\n",
            "chapter\n",
            "compil\n",
            "tank\n",
            "bon\n",
            "volum\n",
            "novemb\n",
            "stori\n",
            "follow\n",
            "adventur\n",
            "monkey\n",
            "luffi\n",
            "crew\n",
            "straw\n",
            "hat\n",
            "pirat\n",
            "explor\n",
            "grand\n",
            "line\n",
            "search\n",
            "mythic\n",
            "treasur\n",
            "known\n",
            "one\n",
            "piec\n",
            "order\n",
            "becom\n",
            "next\n",
            "king\n",
            "pirat\n",
            "manga\n",
            "spawn\n",
            "media\n",
            "franchis\n",
            "adapt\n",
            "festiv\n",
            "film\n",
            "product\n",
            "g\n",
            "anim\n",
            "seri\n",
            "toei\n",
            "anim\n",
            "began\n",
            "broadcast\n",
            "addit\n",
            "toei\n",
            "develop\n",
            "fourteen\n",
            "anim\n",
            "featur\n",
            "film\n",
            "one\n",
            "origin\n",
            "video\n",
            "anim\n",
            "thirteen\n",
            "televis\n",
            "special\n",
            "sever\n",
            "compani\n",
            "develop\n",
            "variou\n",
            "type\n",
            "merchandis\n",
            "media\n",
            "trade\n",
            "card\n",
            "game\n",
            "numer\n",
            "video\n",
            "game\n",
            "manga\n",
            "seri\n",
            "licens\n",
            "english\n",
            "languag\n",
            "releas\n",
            "north\n",
            "america\n",
            "unit\n",
            "kingdom\n",
            "viz\n",
            "media\n",
            "australia\n",
            "madman\n",
            "entertain\n",
            "anim\n",
            "seri\n",
            "licens\n",
            "kid\n",
            "entertain\n",
            "english\n",
            "languag\n",
            "releas\n",
            "north\n",
            "america\n",
            "licens\n",
            "drop\n",
            "subsequ\n",
            "acquir\n",
            "funim\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cpliCBFp3qn",
        "outputId": "bf613744-1be8-4387-ab07-ded99bdd88ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lemmatization\n",
        "\n",
        "for i in corpus :\n",
        "\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edzk26Uep2Lz",
        "outputId": "edcc18ba-6088-4ff0-dda1-1d5a1ea3f85b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one\n",
            "piece\n",
            "stylized\n",
            "cap\n",
            "japanese\n",
            "manga\n",
            "series\n",
            "written\n",
            "illustrated\n",
            "eiichiro\n",
            "oda\n",
            "serialized\n",
            "shueisha\n",
            "sh\n",
            "nen\n",
            "manga\n",
            "magazine\n",
            "weekly\n",
            "sh\n",
            "nen\n",
            "jump\n",
            "since\n",
            "july\n",
            "individual\n",
            "chapter\n",
            "compiled\n",
            "tank\n",
            "bon\n",
            "volume\n",
            "november\n",
            "story\n",
            "follows\n",
            "adventure\n",
            "monkey\n",
            "luffy\n",
            "crew\n",
            "straw\n",
            "hat\n",
            "pirate\n",
            "explores\n",
            "grand\n",
            "line\n",
            "search\n",
            "mythical\n",
            "treasure\n",
            "known\n",
            "one\n",
            "piece\n",
            "order\n",
            "become\n",
            "next\n",
            "king\n",
            "pirate\n",
            "manga\n",
            "spawned\n",
            "medium\n",
            "franchise\n",
            "adapted\n",
            "festival\n",
            "film\n",
            "production\n",
            "g\n",
            "anime\n",
            "series\n",
            "toei\n",
            "animation\n",
            "began\n",
            "broadcasting\n",
            "additionally\n",
            "toei\n",
            "developed\n",
            "fourteen\n",
            "animated\n",
            "feature\n",
            "film\n",
            "one\n",
            "original\n",
            "video\n",
            "animation\n",
            "thirteen\n",
            "television\n",
            "special\n",
            "several\n",
            "company\n",
            "developed\n",
            "various\n",
            "type\n",
            "merchandising\n",
            "medium\n",
            "trading\n",
            "card\n",
            "game\n",
            "numerous\n",
            "video\n",
            "game\n",
            "manga\n",
            "series\n",
            "licensed\n",
            "english\n",
            "language\n",
            "release\n",
            "north\n",
            "america\n",
            "united\n",
            "kingdom\n",
            "viz\n",
            "medium\n",
            "australia\n",
            "madman\n",
            "entertainment\n",
            "anime\n",
            "series\n",
            "licensed\n",
            "kid\n",
            "entertainment\n",
            "english\n",
            "language\n",
            "release\n",
            "north\n",
            "america\n",
            "license\n",
            "dropped\n",
            "subsequently\n",
            "acquired\n",
            "funimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stopwords.Lemmatize\n",
        "\n",
        "import re\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "Vz8TplQMs17m"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#trigrams\n",
        "# cv = CountVectorizer(binary = True, ngram_range=(3,3))\n",
        "\n",
        "# bigrams and trigrams\n",
        "cv = CountVectorizer(binary = True, ngram_range=(2,3))"
      ],
      "metadata": {
        "id": "EU1aKTJKp2Og"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "d0fmQ4Bup2RF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This provides index / feature number not the frequency\n",
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg5JpKKwp2Ug",
        "outputId": "4100c140-7d10-4249-87a3-dd4225150a29"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'one piece': 136,\n",
              " 'piece stylized': 145,\n",
              " 'stylized cap': 179,\n",
              " 'cap japanese': 27,\n",
              " 'japanese manga': 76,\n",
              " 'manga series': 106,\n",
              " 'series written': 162,\n",
              " 'written illustrated': 210,\n",
              " 'illustrated eiichiro': 72,\n",
              " 'eiichiro oda': 45,\n",
              " 'one piece stylized': 138,\n",
              " 'piece stylized cap': 146,\n",
              " 'stylized cap japanese': 180,\n",
              " 'cap japanese manga': 28,\n",
              " 'japanese manga series': 77,\n",
              " 'manga series written': 108,\n",
              " 'series written illustrated': 163,\n",
              " 'written illustrated eiichiro': 211,\n",
              " 'illustrated eiichiro oda': 73,\n",
              " 'serialized shueisha': 155,\n",
              " 'shueisha sh': 169,\n",
              " 'sh nen': 166,\n",
              " 'nen manga': 125,\n",
              " 'manga magazine': 104,\n",
              " 'magazine weekly': 102,\n",
              " 'weekly sh': 208,\n",
              " 'nen jump': 123,\n",
              " 'jump since': 80,\n",
              " 'since july': 171,\n",
              " 'july individual': 78,\n",
              " 'individual chapter': 74,\n",
              " 'chapter compiled': 31,\n",
              " 'compiled tank': 35,\n",
              " 'tank bon': 183,\n",
              " 'bon volume': 25,\n",
              " 'volume november': 207,\n",
              " 'serialized shueisha sh': 156,\n",
              " 'shueisha sh nen': 170,\n",
              " 'sh nen manga': 168,\n",
              " 'nen manga magazine': 126,\n",
              " 'manga magazine weekly': 105,\n",
              " 'magazine weekly sh': 103,\n",
              " 'weekly sh nen': 209,\n",
              " 'sh nen jump': 167,\n",
              " 'nen jump since': 124,\n",
              " 'jump since july': 81,\n",
              " 'since july individual': 172,\n",
              " 'july individual chapter': 79,\n",
              " 'individual chapter compiled': 75,\n",
              " 'chapter compiled tank': 32,\n",
              " 'compiled tank bon': 36,\n",
              " 'tank bon volume': 184,\n",
              " 'bon volume november': 26,\n",
              " 'story follows': 175,\n",
              " 'follows adventure': 60,\n",
              " 'adventure monkey': 5,\n",
              " 'monkey luffy': 119,\n",
              " 'luffy crew': 99,\n",
              " 'crew straw': 37,\n",
              " 'straw hat': 177,\n",
              " 'hat pirate': 70,\n",
              " 'pirate explores': 147,\n",
              " 'explores grand': 50,\n",
              " 'grand line': 68,\n",
              " 'line search': 97,\n",
              " 'search mythical': 153,\n",
              " 'mythical treasure': 121,\n",
              " 'treasure known': 194,\n",
              " 'known one': 87,\n",
              " 'piece order': 143,\n",
              " 'order become': 139,\n",
              " 'become next': 22,\n",
              " 'next king': 127,\n",
              " 'king pirate': 84,\n",
              " 'story follows adventure': 176,\n",
              " 'follows adventure monkey': 61,\n",
              " 'adventure monkey luffy': 6,\n",
              " 'monkey luffy crew': 120,\n",
              " 'luffy crew straw': 100,\n",
              " 'crew straw hat': 38,\n",
              " 'straw hat pirate': 178,\n",
              " 'hat pirate explores': 71,\n",
              " 'pirate explores grand': 148,\n",
              " 'explores grand line': 51,\n",
              " 'grand line search': 69,\n",
              " 'line search mythical': 98,\n",
              " 'search mythical treasure': 154,\n",
              " 'mythical treasure known': 122,\n",
              " 'treasure known one': 195,\n",
              " 'known one piece': 88,\n",
              " 'one piece order': 137,\n",
              " 'piece order become': 144,\n",
              " 'order become next': 140,\n",
              " 'become next king': 23,\n",
              " 'next king pirate': 128,\n",
              " 'manga spawned': 109,\n",
              " 'spawned medium': 173,\n",
              " 'medium franchise': 113,\n",
              " 'franchise adapted': 64,\n",
              " 'adapted festival': 1,\n",
              " 'festival film': 54,\n",
              " 'film production': 58,\n",
              " 'production anime': 149,\n",
              " 'anime series': 17,\n",
              " 'series toei': 160,\n",
              " 'toei animation': 188,\n",
              " 'animation began': 13,\n",
              " 'began broadcasting': 24,\n",
              " 'manga spawned medium': 110,\n",
              " 'spawned medium franchise': 174,\n",
              " 'medium franchise adapted': 114,\n",
              " 'franchise adapted festival': 65,\n",
              " 'adapted festival film': 2,\n",
              " 'festival film production': 55,\n",
              " 'film production anime': 59,\n",
              " 'production anime series': 150,\n",
              " 'anime series toei': 19,\n",
              " 'series toei animation': 161,\n",
              " 'toei animation began': 189,\n",
              " 'animation began broadcasting': 14,\n",
              " 'additionally toei': 3,\n",
              " 'toei developed': 190,\n",
              " 'developed fourteen': 39,\n",
              " 'fourteen animated': 62,\n",
              " 'animated feature': 11,\n",
              " 'feature film': 52,\n",
              " 'film one': 56,\n",
              " 'one original': 134,\n",
              " 'original video': 141,\n",
              " 'video animation': 202,\n",
              " 'animation thirteen': 15,\n",
              " 'thirteen television': 186,\n",
              " 'television special': 185,\n",
              " 'additionally toei developed': 4,\n",
              " 'toei developed fourteen': 191,\n",
              " 'developed fourteen animated': 40,\n",
              " 'fourteen animated feature': 63,\n",
              " 'animated feature film': 12,\n",
              " 'feature film one': 53,\n",
              " 'film one original': 57,\n",
              " 'one original video': 135,\n",
              " 'original video animation': 142,\n",
              " 'video animation thirteen': 203,\n",
              " 'animation thirteen television': 16,\n",
              " 'thirteen television special': 187,\n",
              " 'several company': 164,\n",
              " 'company developed': 33,\n",
              " 'developed various': 41,\n",
              " 'various type': 200,\n",
              " 'type merchandising': 196,\n",
              " 'merchandising medium': 117,\n",
              " 'medium trading': 115,\n",
              " 'trading card': 192,\n",
              " 'card game': 29,\n",
              " 'game numerous': 66,\n",
              " 'numerous video': 132,\n",
              " 'video game': 204,\n",
              " 'several company developed': 165,\n",
              " 'company developed various': 34,\n",
              " 'developed various type': 42,\n",
              " 'various type merchandising': 201,\n",
              " 'type merchandising medium': 197,\n",
              " 'merchandising medium trading': 118,\n",
              " 'medium trading card': 116,\n",
              " 'trading card game': 193,\n",
              " 'card game numerous': 30,\n",
              " 'game numerous video': 67,\n",
              " 'numerous video game': 133,\n",
              " 'series licensed': 157,\n",
              " 'licensed english': 93,\n",
              " 'english language': 46,\n",
              " 'language release': 89,\n",
              " 'release north': 151,\n",
              " 'north america': 129,\n",
              " 'america united': 9,\n",
              " 'united kingdom': 198,\n",
              " 'kingdom viz': 85,\n",
              " 'viz medium': 205,\n",
              " 'medium australia': 111,\n",
              " 'australia madman': 20,\n",
              " 'madman entertainment': 101,\n",
              " 'manga series licensed': 107,\n",
              " 'series licensed english': 158,\n",
              " 'licensed english language': 94,\n",
              " 'english language release': 47,\n",
              " 'language release north': 90,\n",
              " 'release north america': 152,\n",
              " 'north america united': 131,\n",
              " 'america united kingdom': 10,\n",
              " 'united kingdom viz': 199,\n",
              " 'kingdom viz medium': 86,\n",
              " 'viz medium australia': 206,\n",
              " 'medium australia madman': 112,\n",
              " 'australia madman entertainment': 21,\n",
              " 'licensed kid': 95,\n",
              " 'kid entertainment': 82,\n",
              " 'entertainment english': 48,\n",
              " 'america license': 7,\n",
              " 'license dropped': 91,\n",
              " 'dropped subsequently': 43,\n",
              " 'subsequently acquired': 181,\n",
              " 'acquired funimation': 0,\n",
              " 'anime series licensed': 18,\n",
              " 'series licensed kid': 159,\n",
              " 'licensed kid entertainment': 96,\n",
              " 'kid entertainment english': 83,\n",
              " 'entertainment english language': 49,\n",
              " 'north america license': 130,\n",
              " 'america license dropped': 8,\n",
              " 'license dropped subsequently': 92,\n",
              " 'dropped subsequently acquired': 44,\n",
              " 'subsequently acquired funimation': 182}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OygmnSwor5cY",
        "outputId": "80f033fe-be77-4185-8442-e0266f514af8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one piece stylized cap japanese manga series written illustrated eiichiro oda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHxgCG5rtzEW",
        "outputId": "b52bd686-b4dd-4ef9-c7b4-629c24a38475"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  print(X[i].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzY015c-r6MN",
        "outputId": "2946e6ef-01a0-4f94-a785-3dc3799bb4f8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0]]\n",
            "[[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1\n",
            "  1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[[0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[[0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
            "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0]]\n",
            "[[0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1\n",
            "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0]]\n",
            "[[1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv1 = TfidfVectorizer(ngram_range=(2,3))\n",
        "# if the word digram and trigram both are present at least 3 times in frequncy print in the vector\n",
        "cv2 = TfidfVectorizer(ngram_range=(3,3), max_features = 3)\n",
        "\n",
        "# get the top 5 vectors\n",
        "cv3 = TfidfVectorizer(ngram_range=(1,1), max_features = 5)\n",
        "X1 = cv1.fit_transform(corpus)\n",
        "\n",
        "X2 = cv2.fit_transform(corpus)\n",
        "X3 = cv3.fit_transform(corpus)\n"
      ],
      "metadata": {
        "id": "uj2LSAok3oyC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WMQBEXAK3_7T",
        "outputId": "adbf96b7-a1eb-4455-b6e8-f36a2f3652f1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one piece stylized cap japanese manga series written illustrated eiichiro oda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF8TACC53_yz",
        "outputId": "07be4a6a-15f4-48df-852c-94a0dae1a8cc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23309612, 0.23309612, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23309612, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23309612, 0.23309612, 0.        ,\n",
              "        0.        , 0.23309612, 0.23309612, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.19535274, 0.        , 0.23309612, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.19535274, 0.        , 0.23309612, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23309612, 0.23309612, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23309612, 0.23309612, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.23309612,\n",
              "        0.23309612, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23309612, 0.23309612]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOi5SGR-3_wT",
        "outputId": "31b015a8-fd33-4ae1-ddb3-cf43cdaebd5c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X3[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmTl-5x65hh6",
        "outputId": "5daad2b8-890d-4e40-cd1a-18dc5126f0b1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55041302, 0.        , 0.62776669, 0.55041302, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}